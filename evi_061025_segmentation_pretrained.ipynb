{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scativa/IV-Colab/blob/seba/evi_061025_segmentation_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Librerias\n",
        "\n",
        "##functions.py\n",
        "Reemplaza la línea `import functions as func`"
      ],
      "metadata": {
        "id": "ryDySnX4psf8"
      },
      "id": "ryDySnX4psf8"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def load_images(dir, norm=False):\n",
        "    images = [cv2.imread(os.path.join(dir, filename)).astype(np.float32)\n",
        "            for filename in os.listdir(dir)\n",
        "            if cv2.imread(os.path.join(dir, filename)) is not None]\n",
        "    if norm:\n",
        "        images = [image / 255.0 for image in images]\n",
        "\n",
        "    # https://chatgpt.com/c/67fd5724-8164-8006-9367-780189493ee5\n",
        "    images = [cv2.resize(image, (224, 224)).astype(np.float32) for image in images]\n",
        "    return images\n",
        "\n",
        "\n",
        "def load_masks(dir_pores, image_filenames):\n",
        "    masks = [\n",
        "            cv2.imread(os.path.join(dir_pores, filename), cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "            # if os.path.exists(os.path.join(dir_pores, filename))\n",
        "            # else np.zeros((232, 180), dtype=np.float32)\n",
        "            for filename in image_filenames\n",
        "    ]\n",
        "    # masks = [np.expand_dims(mask, axis=-1) for mask in masks] #adds a channel dimension to the masks\n",
        "\n",
        "    # https://chatgpt.com/c/67fd5724-8164-8006-9367-780189493ee5\n",
        "    masks = [cv2.resize(mask, (224, 224)).astype(np.float32) for mask in masks]\n",
        "\n",
        "    return masks\n",
        "\n",
        "# just to plot masks\n",
        "def plot_img_mask(img, mask, pred):\n",
        "\n",
        "    sub_plot = 0\n",
        "    num_cols = 0\n",
        "\n",
        "    # Determina el número de columnas en función de si hay una predicción\n",
        "    if img is not None:\n",
        "        num_cols += 1\n",
        "    if mask is not None:\n",
        "        num_cols += 1\n",
        "    if pred is not None:\n",
        "        num_cols += 1\n",
        "\n",
        "    # Si no hay nada para mostrar, sal de la función\n",
        "    if num_cols == 0:\n",
        "        print(\"No hay nada para mostrar.\")\n",
        "        return\n",
        "\n",
        "    # Crea la figura y los ejes en una sola fila con el número determinado de columnas\n",
        "    fig, axes = plt.subplots(1, num_cols, figsize=(3, 3))\n",
        "\n",
        "    if (img is not None):\n",
        "      axes[sub_plot].imshow(img)\n",
        "      axes[sub_plot].set_title('Image')\n",
        "      axes[sub_plot].axis('off')\n",
        "      sub_plot += 1\n",
        "\n",
        "    if (mask is not None):\n",
        "      axes[sub_plot].imshow(mask, cmap='gray')\n",
        "      axes[sub_plot].set_title('Mask')\n",
        "      axes[sub_plot].axis('off')\n",
        "      sub_plot += 1\n",
        "\n",
        "    if (pred is not None):\n",
        "      pred_mask = np.where(pred <= 0.5, 0, 1)\n",
        "\n",
        "      axes[sub_plot].imshow(pred_mask, cmap='gray')\n",
        "      axes[sub_plot].set_title('Predicted')\n",
        "      axes[sub_plot].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def augmented_dataset(dir_images, dir_mask, seed, norm):\n",
        "    # From \"Pores 03/28/25 -Seba- Basado en segmentation_own (DTE_fisuras).ipynb\"\n",
        "\n",
        "\n",
        "    images = load_images(dir_images, norm)\n",
        "    # Get image filenames from loaded images\n",
        "\n",
        "    image_filenames = [os.path.basename(image_path) for image_path in os.listdir(dir_images)]\n",
        "    # masks = load_masks(dir_pores)\n",
        "    masks = load_masks(dir_mask, image_filenames)\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=seed)\n",
        "\n",
        "    X_train[0].shape\n",
        "    y_train[0].shape\n",
        "    augmented_X_train = [cv2.flip(i, 1) for i in X_train]\n",
        "    augmented_y_train = [cv2.flip(i, 1) for i in y_train]\n",
        "\n",
        "    X_train = np.array(X_train + augmented_X_train)\n",
        "    y_train = np.array(y_train + augmented_y_train)\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n",
        "\n",
        "    y_train = np.expand_dims(y_train, axis=-1)\n",
        "    y_val = np.expand_dims(y_val, axis=-1)\n",
        "    y_test = np.expand_dims(y_test, axis=-1)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "def test_dataset(dir_images, dir_mask, seed, norm):\n",
        "    # From \"Pores 03/28/25 -Seba- Basado en segmentation_own (DTE_fisuras).ipynb\"\n",
        "\n",
        "\n",
        "    images = load_images(dir_images, norm)\n",
        "    # Get image filenames from loaded images\n",
        "\n",
        "    image_filenames = [os.path.basename(image_path) for image_path in os.listdir(dir_images)]\n",
        "    # masks = load_masks(dir_pores)\n",
        "    masks = load_masks(dir_mask, image_filenames)\n",
        "\n",
        "\n",
        "    _, X_test, _, y_test = train_test_split(images, masks, test_size=0.2, random_state=seed)\n",
        "\n",
        "    y_test = np.expand_dims(y_test, axis=-1)\n",
        "\n",
        "    return X_test, y_test\n",
        "\n",
        "'''\n",
        "If you want to show some other metrics, just uncomment\n",
        "'''\n",
        "def plot_history(history, x_lim, y_lim):\n",
        "    #val_iou_score = history.history['val_iou_score']\n",
        "    #iou_score = history.history['iou_score']\n",
        "    val_f1_score = history.history['val_f1-score']\n",
        "    #f1_score = history.history['f1-score']\n",
        "    val_loss = history.history['val_loss']\n",
        "    loss = history.history['loss']\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    #plt.plot(epochs, val_iou_score, 'b', label='Validation IoU Score')\n",
        "    #plt.plot(epochs, iou_score, 'g', label='IoU Score')\n",
        "    #plt.plot(epochs, f1_score, 'r', label='F1 Score')\n",
        "    plt.plot(epochs, val_f1_score, 'c', label='Validation F1 Score')\n",
        "    plt.plot(epochs, val_loss, 'm', label='Validation Loss')\n",
        "    plt.plot(epochs, loss, 'y', label='Training Loss')\n",
        "\n",
        "    plt.title('Training Metrics', fontsize=16)\n",
        "    plt.xlabel('Epochs', fontsize=14)\n",
        "    plt.ylabel('Score/Loss', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.xlim(1, x_lim)\n",
        "    plt.ylim(0, y_lim)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "H80FZsTspq9N"
      },
      "id": "H80FZsTspq9N",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entorno"
      ],
      "metadata": {
        "id": "Y6FbynbEtxDc"
      },
      "id": "Y6FbynbEtxDc"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jLY5vQB7tS_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60273fa3-190c-4c88-e03c-bb3fe2acf9cd"
      },
      "id": "jLY5vQB7tS_O",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import process_time_ns\n",
        "import os, json\n",
        "\n",
        "# usr_drive = \"evi\"\n",
        "usr_drive = \"ppca.cnea\"\n",
        "\n",
        "\n",
        "base_paths = { # Path al\n",
        "    \"ppca.cnea\": '/content/drive/MyDrive/IV',\n",
        "    \"seba.cnea\": '/content/drive/MyDrive/CNEA/DCA/Proyectos/IV - Inspección Visual/IV',\n",
        "    \"scativa\": '/content/drive/MyDrive/Laburo/CNEA/DCA/Proyectos/IV - Inspección Visual/IV',\n",
        "    \"evi\": '/content/drive/MyDrive/IV',\n",
        "}\n",
        "# base_path =\n",
        "base_path = base_paths[usr_drive]\n",
        "\n",
        "if not os.path.exists(base_path):\n",
        "  print(f'Carpeta inexistente \"{usr_drive}: {base_path}\"')\n",
        "else:\n",
        "  print(f'Utilizando carpeta  \"{usr_drive}: {base_path}\"')\n",
        "\n",
        "\n",
        "#import tensorflow as tf\n",
        "# Setting GPU memory growth to True\n",
        "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "#if gpus:\n",
        "#    print(f\"GPUs detected: {gpus}\")\n",
        "#    try:\n",
        "#        for gpu in gpus:\n",
        "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#    except RuntimeError as e:\n",
        "#        print(e)\n",
        "\n",
        "\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n"
      ],
      "metadata": {
        "id": "u8K_GpqhP4P1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a0e33c-9405-46da-d345-a3f4fb6b0d68"
      },
      "id": "u8K_GpqhP4P1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando carpeta  \"ppca.cnea: /content/drive/MyDrive/IV\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "Tu8wymLMQsy6"
      },
      "id": "Tu8wymLMQsy6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0f79518c-640f-483d-8465-4fa8fc47e4ad",
      "metadata": {
        "id": "0f79518c-640f-483d-8465-4fa8fc47e4ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c708df0-018c-4580-84e5-cdb43d66073f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando carpeta  \"ppca.cnea: /content/drive/MyDrive/IV\"\n",
            "Utilizando carpeta  \"ppca.cnea: /content/drive/MyDrive/IV\"\n"
          ]
        }
      ],
      "source": [
        "defect = 'cachaduras' # 'pores'\n",
        "\n",
        "image_directory = f'{base_path}/Images/Output_imágenes y máscaras 20250318/pellets'\n",
        "masks_directory = f'{base_path}/Images/Output_imágenes y máscaras 20250318/Output_052025 (máscaras actualizadas)/all_{defect}'\n",
        "\n",
        "if not os.path.exists(image_directory):\n",
        "  print(f'Carpeta inexistente \"{usr_drive}: {base_path}\"')\n",
        "else:\n",
        "  print(f'Utilizando carpeta  \"{usr_drive}: {base_path}\"')\n",
        "\n",
        "if not os.path.exists(masks_directory):\n",
        "  print(f'Carpeta inexistente \"{usr_drive}: {base_path}\"')\n",
        "else:\n",
        "  print(f'Utilizando carpeta  \"{usr_drive}: {base_path}\"')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === MODO DE EJECUCIÓN ===\n",
        "# Usar 'cargar' para usar los datos ya procesados guardados (.npy)\n",
        "# Usar 'generar' para crear nuevos datos y guardarlos a partir de las imágenes originales\n",
        "modo = 'generar'\n",
        "\n",
        "# === Rutas ===\n",
        "save_dir = f'{base_path}/processed_data_{defect}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# === Cargar o generar los datos ===\n",
        "if modo == 'cargar':\n",
        "    print(\"Cargando datos preprocesados desde archivos .npy...\")\n",
        "    X_train = np.load(os.path.join(save_dir, 'X_train.npy'))\n",
        "    X_val   = np.load(os.path.join(save_dir, 'X_val.npy'))\n",
        "    X_test  = np.load(os.path.join(save_dir, 'X_test.npy'))\n",
        "    y_train = np.load(os.path.join(save_dir, 'y_train.npy'))\n",
        "    y_val   = np.load(os.path.join(save_dir, 'y_val.npy'))\n",
        "    y_test  = np.load(os.path.join(save_dir, 'y_test.npy'))\n",
        "    print(\"✅ Datos cargados correctamente.\")\n",
        "\n",
        "elif modo == 'generar':\n",
        "    print(\"Generando nuevos datos a partir de imágenes y máscaras...\")\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = augmented_dataset(\n",
        "        image_directory, masks_directory, seed=42, norm=False\n",
        "    )\n",
        "\n",
        "    # Guardar los datos generados\n",
        "    np.save(os.path.join(save_dir, 'X_train.npy'), X_train)\n",
        "    np.save(os.path.join(save_dir, 'X_val.npy'),   X_val)\n",
        "    np.save(os.path.join(save_dir, 'X_test.npy'),  X_test)\n",
        "    np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
        "    np.save(os.path.join(save_dir, 'y_val.npy'),   y_val)\n",
        "    np.save(os.path.join(save_dir, 'y_test.npy'),  y_test)\n",
        "\n",
        "    print(f\"✅ Nuevos datos generados y guardados en {save_dir}\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"❌ Modo inválido. Usa 'cargar' o 'generar'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "E6zrvrCuVize",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5426df35-5f08-4a51-d829-2f3b15005a87"
      },
      "id": "E6zrvrCuVize",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando nuevos datos a partir de imágenes y máscaras...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Muestra de imagenes y máscaras cargadas\n",
        "\n",
        "import random\n",
        "\n",
        "print(\"Train: \", len(X_train), len(y_train))\n",
        "print(\"Test: \", len(X_test), len(y_test))\n",
        "print(\"Val: \", len(X_val), len(y_val))\n",
        "\n",
        "pos = random.randint(0, len(X_train))\n",
        "plot_img_mask(np.int64(X_train[pos]), y_train[pos], None)\n",
        "\n",
        "pos = random.randint(0, len(X_test))\n",
        "plot_img_mask(np.int64(X_test[pos]), y_test[pos], None)\n",
        "\n",
        "pos = random.randint(0, len(X_val))\n",
        "plot_img_mask(np.int64(X_val[pos]), y_val[pos], None)\n",
        "\n",
        "print(\"Train: \", type(X_train), 'input range: ', np.min(X_train), np.max(X_train))\n",
        "print(\"Val: \", type(X_val), 'input range: ', np.min(X_val), np.max(X_val))\n",
        "print(\"Test: \", type(X_test), 'input range: ', np.min(X_test), np.max(X_test))\n"
      ],
      "metadata": {
        "id": "jVyyNChjPAzz"
      },
      "id": "jVyyNChjPAzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selección del modelo y preprocesamiento de data"
      ],
      "metadata": {
        "id": "lxOBAMVrW_Nv"
      },
      "id": "lxOBAMVrW_Nv"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models\n",
        "import segmentation_models as sm\n",
        "from segmentation_models import get_preprocessing\n",
        "\n",
        "\n",
        "model_type = \"vgg19\"\n",
        "# \"resnet152\" \"efficientnetb2\" \"vgg19\"\n",
        "print(f'Cargando modelo: {model_type}')\n",
        "\n",
        "preprocessed_input = get_preprocessing(model_type)\n",
        "\n",
        "# Preprocesar las imágenes\n",
        "X_train_preprocessed = preprocessed_input(np.copy(X_train))\n",
        "X_val_preprocessed = preprocessed_input(np.copy(X_val))\n",
        "X_test_preprocessed = preprocessed_input(np.copy(X_test))\n",
        "\n",
        "print(\"Train: \", type(X_train_preprocessed), 'input range: ', np.min(X_train_preprocessed), np.max(X_train_preprocessed))\n",
        "print(\"Val: \", type(X_val_preprocessed), 'input range: ', np.min(X_val_preprocessed), np.max(X_val_preprocessed))\n",
        "print(\"Test: \", type(X_test_preprocessed), 'input range: ', np.min(X_test_preprocessed), np.max(X_test_preprocessed))\n"
      ],
      "metadata": {
        "id": "URw3-ncOBoXz"
      },
      "id": "URw3-ncOBoXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento"
      ],
      "metadata": {
        "id": "ZvTCeVrIXy8n"
      },
      "id": "ZvTCeVrIXy8n"
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "\n",
        "print(f\"🔧 Usando modelo: {model_type}\")\n",
        "\n",
        "# === Registro para serialización ===\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
        "def custom_bce_dice_loss(y_true, y_pred):\n",
        "    return sm.losses.bce_dice_loss(y_true, y_pred)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
        "def custom_iou_score(y_true, y_pred):\n",
        "    return sm.metrics.iou_score(y_true, y_pred)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
        "def custom_f1_score(y_true, y_pred):\n",
        "    return sm.metrics.f1_score(y_true, y_pred)\n",
        "\n",
        "# === 2. Rutas ===\n",
        "checkpoint_path = f\"{base_path}/model/checkpoints_{model_type}_{defect}/{model_type}_last.keras\"\n",
        "history_pkl_path = f\"{base_path}/model/checkpoints_{model_type}_{defect}/{model_type}_history.pkl\"\n",
        "history_csv_path = f\"{base_path}/model/checkpoints_{model_type}_{defect}/{model_type}_history.csv\"\n",
        "out_folder = f\"{base_path}/output/model_{defect}_{model_type}\"\n",
        "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "# === 3. Callback mejorado para guardar historial .pkl y actualizar .csv ===\n",
        "class HistorySaver(Callback):\n",
        "    def __init__(self, pkl_path, csv_path):\n",
        "        super().__init__()\n",
        "        self.pkl_path = pkl_path\n",
        "        self.csv_path = csv_path\n",
        "\n",
        "        # Cargar historial CSV existente si hay\n",
        "        if os.path.exists(csv_path):\n",
        "            self.df_history = pd.read_csv(csv_path, index_col=\"epoch\")\n",
        "        else:\n",
        "            self.df_history = pd.DataFrame()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        new_row = pd.DataFrame([logs], index=[epoch])\n",
        "        new_row.index.name = 'epoch'\n",
        "\n",
        "        # Agregar nueva fila al historial existente\n",
        "        self.df_history = pd.concat([self.df_history, new_row])\n",
        "        self.df_history = self.df_history[~self.df_history.index.duplicated(keep='last')]\n",
        "        self.df_history.sort_index(inplace=True)\n",
        "\n",
        "        # Guardar como CSV\n",
        "        self.df_history.to_csv(self.csv_path)\n",
        "        print(f\"📝 CSV actualizado hasta época {epoch + 1}\")\n",
        "\n",
        "        # También guardar como .pkl para reanudar\n",
        "        with open(self.pkl_path, 'wb') as f:\n",
        "            pickle.dump(self.df_history.to_dict(orient='list'), f)\n",
        "\n",
        "# === 4. Función para verificar historial válido ===\n",
        "def valid_history(hist):\n",
        "    return (\n",
        "        isinstance(hist, dict) and\n",
        "        'loss' in hist and\n",
        "        isinstance(hist['loss'], list) and\n",
        "        len(hist['loss']) > 0\n",
        "    )\n",
        "\n",
        "# === 5. Cargar o crear modelo ===\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"🔁 Cargando modelo completo desde: {checkpoint_path}\")\n",
        "    model = tf.keras.models.load_model(\n",
        "        checkpoint_path,\n",
        "        custom_objects={\n",
        "            'custom_bce_dice_loss': custom_bce_dice_loss,\n",
        "            'custom_iou_score': custom_iou_score,\n",
        "            'custom_f1_score': custom_f1_score\n",
        "        }\n",
        "    )\n",
        "else:\n",
        "    print(\"🆕 Creando nuevo modelo...\")\n",
        "    model = sm.Unet(\n",
        "        backbone_name=model_type,\n",
        "        classes=1,\n",
        "        activation='sigmoid',\n",
        "        encoder_weights='imagenet'\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer='Adam',\n",
        "        loss=custom_bce_dice_loss,\n",
        "        metrics=[custom_iou_score, custom_f1_score]\n",
        "    )\n",
        "\n",
        "# === 6. Cargar historial si existe ===\n",
        "if os.path.exists(history_pkl_path):\n",
        "    with open(history_pkl_path, 'rb') as f:\n",
        "        previous_history = pickle.load(f)\n",
        "    if valid_history(previous_history):\n",
        "        print(f\"📈 Historial cargado con {len(previous_history['loss'])} épocas.\")\n",
        "    else:\n",
        "        previous_history = None\n",
        "        os.remove(history_pkl_path)\n",
        "        print(\"⚠️ Historial inválido. Se iniciará desde cero.\")\n",
        "else:\n",
        "    previous_history = None\n",
        "    print(\"📉 No se encontró historial previo.\")\n",
        "\n",
        "initial_epoch = len(previous_history['loss']) if previous_history else 0\n",
        "\n",
        "# === 7. Entrenamiento ===\n",
        "batch_size = 16\n",
        "epochs = 500\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True),\n",
        "    ModelCheckpoint(checkpoint_path, save_best_only=False),\n",
        "    HistorySaver(history_pkl_path, history_csv_path)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    x=X_train_preprocessed,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    initial_epoch=initial_epoch,\n",
        "    validation_data=(X_val_preprocessed, y_val),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# === 8. Guardar modelo final completo ===\n",
        "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "final_model_path = f'{out_folder}/{now}-{model_type}-{defect}.keras'\n",
        "model.save(final_model_path)\n",
        "print(f\"✅ Modelo final guardado: {final_model_path}\")\n",
        "\n",
        "# === 9. Evaluación ===\n",
        "print(\"\\n📊 Validación:\")\n",
        "model.evaluate(X_val_preprocessed, y_val)\n",
        "\n",
        "print(\"\\n📊 Test:\")\n",
        "model.evaluate(X_test_preprocessed, y_test)\n",
        "\n",
        "# === 10. Predicción ===\n",
        "pred = model.predict(X_test_preprocessed)\n"
      ],
      "metadata": {
        "id": "cZEd7XltcE9E"
      },
      "id": "cZEd7XltcE9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === 1. Generar predicciones (si no lo hiciste ya) ===\n",
        "y_pred = model.predict(X_test_preprocessed)\n",
        "\n",
        "# Define the base directory for output and predictions\n",
        "output_dir = f\"{base_path}/output\"\n",
        "predictions_dir = os.path.join(output_dir, f\"predictions_{defect}_{model_type}\")\n",
        "results_dir = os.path.join(predictions_dir, f\"visualizations_{defect}_{model_type}\") # Keep visualization directory as before\n",
        "\n",
        "# Create the necessary directories if they don't exist\n",
        "os.makedirs(predictions_dir, exist_ok=True)\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Guardar predicciones como .npy ===\n",
        "np.save(os.path.join(predictions_dir, \"y_pred_test.npy\"), y_pred)\n",
        "print(\"✅ Predicciones guardadas.\")\n",
        "\n",
        "# === 3. Visualizar y guardar comparaciones ===\n",
        "# Escala las máscaras para visualización (umbral opcional)\n",
        "threshold = 0.5\n",
        "y_pred_bin = (y_pred > threshold).astype(np.uint8)\n",
        "\n",
        "# Iterar sobre algunas muestras (puedes limitar si es mucho)\n",
        "for i in range(len(X_test)):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    original_image_display = np.clip(X_test[i], 0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "    # Imagen original\n",
        "    axes[0].imshow(original_image_display)\n",
        "    axes[0].set_title(\"Imagen original\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Ground truth\n",
        "    axes[1].imshow(y_test[i].squeeze(), cmap='gray')\n",
        "    axes[1].set_title(\"Máscara Real\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # Predicción\n",
        "    axes[2].imshow(y_pred_bin[i].squeeze(), cmap='gray')\n",
        "    axes[2].set_title(\"Predicción\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(results_dir, f\"test_{i:03d}.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "print(f\"🖼️ Visualizaciones guardadas en: {results_dir}\")"
      ],
      "metadata": {
        "id": "g9NfDeNlIzVB"
      },
      "id": "g9NfDeNlIzVB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}